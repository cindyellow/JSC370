---
title: "JSC370_HW3"
author: "Shih-Ting (Cindy) Huang"
date: "03/17/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r}
library(tidytext)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(kableExtra)
library(httr)
library(stringr)
```

# APIs

## Retrieve details
```{r}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+vaccine")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
total_count <- str_extract(counts, "[0-9,]+")
```

By extracting with the assistance of NCBI API, we discover that there are a total of `r total_count` papers under the search term "sars-cov-2 vaccine". We then proceed to find the details of these papers.

```{r}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(db='pubmed',
               term = 'sars-cov-2+vaccine',
               retmax = 1000)
)

# Extracting the content of the response of GET
ids <- httr::content(query_ids)
```

```{r}
ids <- as.character(ids)

ids <- str_extract_all(ids, "<Id>[0-9]+</Id>")[[1]]

ids <- str_remove_all(ids, "<Id>|</Id>")

ids<-head(ids, 250)
```

## Download paper details

After retrieving the ids of aforementioned papers in Pubmed, we download the details of the first 250 papers. 

```{r, echo=TRUE}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(db='pubmed',
               id=paste(ids,collapse=","),
               retmax = 250,
               rettype="abstract"
               )
)
```

```{r}
publications <- httr::content(publications)
publications_txt <- as.character(publications)
```

## Create a Dataset

For each paper, we extracted the following details:

* Pubmed ID Number
* Title of the paper
* Name of the journal that published it
* Publication date
* Abstract

If a specific information isn't available, it is represented by `NA` in the dataset.

```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
```

```{r}
titles <- str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
```

```{r}
names <- str_extract(pub_char_list, "<Journal>(\\n|.)+</Journal>")
names <- str_extract(names, "<Title>(\\n|.)+</Title>")
names <- str_remove_all(names, "</?[[:alnum:]]+>")
```

```{r}
dates <- str_extract(pub_char_list, "<Journal>(\\n|.)+</Journal>")
dates <- str_extract(dates, "<PubDate>(\\n|.)+</PubDate>")
year <- str_extract(dates, "<Year>(\\n|.)+</Year>")
year <- str_remove_all(year, "</?[[:alnum:]]+>")
month <- str_extract(dates, "<Month>(\\n|.)+</Month>")
month <- str_remove_all(month, "</?[[:alnum:]]+>")
day <- str_extract(dates, "<Day>(\\n|.)+</Day>")
day <- str_remove_all(day, "</?[[:alnum:]]+>")

pub_date <- paste(year, month, day, sep=" ")
pub_date <- str_remove_all(pub_date, "NA")
pub_date <- trimws(pub_date)
pub_date <- str_replace(pub_date, "^$", replacement = NA_character_)
```

```{r}
abstracts <- str_extract(pub_char_list, "<Abstract>(\\n|.)+</Abstract>")
# Clean HTML tags
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]]+>")
# Remove the Abstract section labels
abstracts <- str_remove_all(abstracts, "<AbstractText .*>")
# Clear extra white space and new lines
abstracts <- str_replace_all(abstracts, "//s+", " ")
```

```{r}
database <- data.frame(
  PubMedID = ids,
  Title = titles,
  "Journal Name" = names,
  "Publication Date" = pub_date,
  Abstract = abstracts
)
database %>%
  kable %>%
  kable_styling("striped", full_width = F) %>% 
 scroll_box(width = "100%", height = "500px")
```

# Text Mining

## Question 1

```{r}
pubmed <- read_csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/text/pubmed.csv")
```

First, let's take a look at the tokens before removing stop words.

```{r}
pubmed %>%
  select(abstract) %>%
  # output col is word, input is abstract
  unnest_tokens(word, abstract) %>%
  group_by(word) %>%
  summarise(word_frequency = n()) %>%
  # arrange by word frequency from most frequent
  arrange(across(word_frequency, desc)) %>%
  head(20) %>%
  
  ggplot(aes(x = reorder(word, word_frequency), y = word_frequency)) +
  # stat means use y column values directly instead of counting
  geom_bar(stat="identity", fill = "#384D48") + coord_flip() +
  ylab("Frequency") +
  xlab("Word") +
  labs(title = "General Word Frequency") +
  theme_minimal()
```

The top 5 most common words are, as shown, "the", "of", "and", "in", and "to", which is expected. However, these are not very useful for analysis, so next we will see what the most frequent tokens are after removing stop words.

```{r}
pubmed %>%
  select(abstract) %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words, by="word") %>%
  group_by(word) %>%
  summarise(word_frequency = n()) %>%
  # arrange by word frequency from most frequent
  arrange(across(word_frequency, desc)) %>%
  head(20) %>%
  
  ggplot(aes(x = reorder(word, word_frequency), y = word_frequency)) +
  # stat means use y column values directly instead of counting
  geom_bar(stat="identity", fill = "#C4DACF") + coord_flip() +
  ylab("Frequency") +
  xlab("Word") +
  labs(title = "General Word Frequency (without stop words)") +
  theme_minimal()
```

After removing stop words, we find that the top 5 most frequent tokens are "covid", "19", "patients", "cancer", and "prostate". 

```{r}
pubmed %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words, by = "word") %>%
  subset(!grepl("[2-9|0][0-8]+", word)) %>%
  group_by(term) %>%
  count(word, sort=T) %>%
  # Choose the top 1
  top_n(1,n) %>%
  
  ggplot(aes(x=reorder(term, n), y = n, fill=word)) +
  geom_bar(stat="identity") +
  coord_flip() +
  scale_fill_manual(values = c("#384D48", "#C4E7D4", "#C4DACF","#B9C0DA", "#998DA0")) +
  ylab("Frequency") +
  xlab("Search Term") +
  labs(title = "Word Frequency by Search Term") +
  theme_minimal()
```

In the graph above, we see the top token for each search term. It is interesting that for terms "covid", "prostate cancer", "preeclampsia", and "cystic fibrosis", all the tokens are related to the term itself whereas for "meningitis", the most frequent token is "patients".

```{r}
term_top_5 <- pubmed %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words, by = "word") %>%
  subset(!grepl("[2-9|0][0-8]+", word)) %>%
  group_by(term) %>%
  count(word, sort=T) %>%
  # Choose the top 1
  top_n(5,n)

# Order by search terms
term_top_5 <- term_top_5[order(term_top_5$term),]

kable(term_top_5, col.names = c("Search Term", "Token", "Count"), caption = "Top 5 Tokens for Each Search Term") %>%
  kable_styling("striped", full_width = T, position="center") %>% 
 scroll_box(width = "100%", height = "500px")
```


In a table, we then demonstrate the 5 most common tokens for each search term after removing stop words. 

## Question 2

For bigrams, we have chosen to remove those that contains numbers other than "19", which is a contextually important number for COVID-19.

```{r}
tokens_bigram <- pubmed %>%
  select(abstract) %>%
  unnest_tokens(bigram, abstract, token="ngrams", n=2) %>%
  group_by(bigram) %>%
  summarise(bigram_frequency = n()) %>%
  separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep = " ", fill = "right") %>%
  # do on word1 and  word2 separately, "wordx" is the col name in tokens, "word" is the col name in stop_words
  anti_join(stop_words, by = c("word1"= "word")) %>%
  anti_join(stop_words, by = c("word2"="word")) %>%
  subset(!grepl("[2-9|0][0-8]+", bigram)) %>%
  arrange(across(bigram_frequency, desc))

tokens_bigram %>%
  head(10) %>%
  ggplot(aes(x=reorder(bigram, bigram_frequency), y=bigram_frequency)) +
  geom_bar(stat="identity", fill = "#B9C0DA") + 
  xlab("Frequency") +
  ylab("Bigram") +
  theme_minimal()+
  coord_flip() +
  labs(title = "Bigram Frequency")
```

Demonstrated above are the top 10 most common bigrams among the abstracts. We see that results related to COVID-19 occupy 7 of the top 10, demonstrating the significance of this topic and its prominence in the dataset.

```{r}
tokens_bigram_term <- pubmed %>%
  unnest_tokens(bigram, abstract, token="ngrams", n=2) %>%
  separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep = " ", fill = "right") %>%
  anti_join(stop_words, by = c("word1"= "word")) %>%
  anti_join(stop_words, by = c("word2"="word")) %>%
  subset(!grepl("[2-9|0][0-8]+", bigram)) %>%
  group_by(term) %>%
  count(bigram, sort=T) %>%
  top_n(1,n)

tokens_bigram_term %>%
  ggplot(aes(x=reorder(term, n), y = n, fill = bigram)) +
  geom_bar(stat="identity") +
  scale_fill_manual(values = c("#384D48", "#C4E7D4", "#C4DACF","#B9C0DA", "#998DA0")) +
  coord_flip() +
  ylab("Frequency") +
  xlab("Search Term") +
  labs(title="Bigram Frequency by Search Term") +
  theme_minimal()
```

We also take a look at the most common bigram for each search term, finding that they all correspond to the search term except "cerebrospinal fluid" for search term "meningitis".

## Question 3

```{r}
tfidf_top_5 <- pubmed %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words, by = "word") %>%
  count(word, term) %>%
  bind_tf_idf(word, term, n) %>%
  group_by(term) %>%
  top_n(5,n) %>%
  arrange(term, desc(tf_idf))

kable(tfidf_top_5, col.names = c("Word", "Search Term", "Number of Observations", "TF", "IDF", "TF_IDF"), caption = "Top 5 Tokens with Highest TF-IDF by Search Term") %>%
  kable_styling("striped", full_width = T, position="center") %>% 
 scroll_box(width = "100%", height = "500px")
```


For all the search terms, the same words appear in the top 5 here as in Q1. However, we can spot several differences from the former results. For example, in the search term "covid", the word "pandemic" has become more important despite its relatively lower count. Similar changes in word importance occurred for other search terms, with the more professional/medical terms gaining more importance and receiving a higher TF-IDF score.






