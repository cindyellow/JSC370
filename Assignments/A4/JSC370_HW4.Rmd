---
title: "JSC370 - HW4"
author: "Shih-Ting (Cindy) Huang"
date: "3/26/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)
```

# HPC
## Problem 1
```{r}
# Total row sums
fun1 <- function(mat){
  n <- nrow(mat)
  ans <- double(n)
  for (i in 1:n){
    ans[i] <- sum(mat[i, ])
  }
  ans
}

fun1alt <- function(mat){
  rowSums(mat)
}
```

```{r}
fun2 <- function(mat){
  n <- nrow(mat)
  k <- ncol(mat)
  ans <- mat
  for (i in 1:n){
    for (j in 2:k){
      ans[i,j] <- mat[i,j] + ans[i,j-1]
    }
  }
  ans
}

fun2alt <- function(mat){
  t(apply(mat, 1, cumsum))
}
```

```{r}
# Use the data with this code
set.seed(250)
dat <- matrix(rnorm(200*10), nrow=200)

# Test for the first
microbenchmark::microbenchmark(
  fun1(dat),
  fun1alt(dat), check="equivalent"
)

# Test for the second
microbenchmark::microbenchmark(
  fun2(dat),
  fun2alt(dat), check="equivalent"
)
```

## Problem 2
```{r}
sim_pi <- function(n=1000, i=NULL){
  p <- matrix(runif(n*2), ncol=2)
  mean(rowSums(p^2) < 1) * 4
}

set.seed(156)
sim_pi(1000)
```

```{r}
set.seed(1231)
system.time({
  ans <- unlist(lapply(1:4000, sim_pi, n = 10000))
  print(mean(ans))
})
```

```{r}
# MY CODE HERE
system.time({
  # MY CODE HERE
  ans <- # MY CODE HERE
  print(mean(ans))
  # MY CODE HERE
})
```

# ML
```{r}
hitters <-read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/hitters/hitters.csv")

set.seed(69)
# Separate into 70% training 30% testing set
train = sample(1:nrow(hitters), floor(nrow(hitters) * 0.7))
test = setdiff(1:nrow(hitters), train)
```

```{r}
head(hitters)
```

## Step 1: Regression Tree
```{r}
hitter_tree <- rpart(Salary ~ ., data=hitters[train,], method="anova")
rpart.plot(hitter_tree)
```

```{r}
plotcp(hitter_tree)
printcp(hitter_tree)

optimalcp <- hitter_tree$cptable[which.min(hitter_tree$cptable[, "xerror"]), 'CP']

hitter_tree_prune <- prune(hitter_tree, cp=optimalcp)

rpart.plot(hitter_tree_prune)
```
## Step 2: Bagging
```{r}
hitter_bag <- randomForest(Salary~., data=hitters[train,], mtry=19, na.action=na.omit)

sum(hitter_bag$err.rate[,1])

varImpPlot(hitter_bag, n.var=19, col="red")
```

## Step 3: Random Forest
```{r}
hitter_rf <- randomForest(Salary~., data=hitters[train,], na.action=na.omit)

sum(hitter_rf$err.rate[,1])

varImpPlot(hitter_rf, n.var=19, col="green")
importance(hitter_rf)
```

```{r}
head(hitters)
sum(is.na(hitters))
```

## Step 4: Boosting
```{r}
# Omit NA values
hitters <-read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/hitters/hitters.csv")
set.seed(1060)
hitters <-na.omit(hitters)
hitters$Division <- as.factor(hitters$Division)
hitters$League <- as.factor(hitters$League)
hitters$NewLeague <- as.factor(hitters$NewLeague)

train = sample(1:nrow(hitters), floor(nrow(hitters) * 0.7))
test = setdiff(1:nrow(hitters), train)

# Use gaussian loss function to minimize squared error
lambdas <- c(0.0001, 0.001, 0.0025, 0.01, 0.1, 0.3, 0.5, 1, 1.5)
mses <- NULL
for (lambda in lambdas){
   hitter_boost = gbm(Salary~., data = hitters[train,], distribution="gaussian", n.trees=5000, shrinkage=lambda)
  
  yhat_boost <- predict(hitter_boost, newdata=hitters[train,], n.trees=1000)
  
  mses <- rbind(mses, mean((yhat_boost-hitters[train,"Salary"])^2))
}
plot(lambdas, mses)
```

Using the model with the best shrinkage parameter on the training set ($\lambda = 1$), we construct the variable importance plot. 

```{r}
hitter_boost = gbm(Salary~., data = hitters[train,], distribution="gaussian", n.trees=5000, shrinkage=1)

summary(hitter_boost)
```

## Step 5: XGBoost
```{r}
# Using caret because it enables us to use grid search on XGBoost
train_control = trainControl(method="cv", number=10, search="grid")

# Create the parameter combinations for Grid Search
tune_grid <- expand.grid(max_depth=c(1,3,5,7),
                         nrounds=(1:10) * 50,
                         eta=c(0.01,0.1,0.3),
                         gamma=0, # minimum amount of reduction
                         subsample=c(1,5,10,20),
                         min_child_weight=c(1,5,10),
                         colsample_bytree = c(0.6, 0.8, 1.0) # number of columns that get subsampled
                         )

# Will save the results we've gotten so far even if it didn't finish running
hitter_xgb <- caret::train(Salary~., data=hitters[train,], method="xgbTree",trControl=train_control,tuneGrid=tune_grid)

plot(varImp(hitter_xgb,scale=F))

yhat_xgb <- predict(hitter_xgb, newdata=hitters[train,])

# Automatically calculates RMSE
caret::RMSE(hitters[train,"Salary"], yhat_xgb)^2
```


## Step 6: Compare Performance
```{r}

```





