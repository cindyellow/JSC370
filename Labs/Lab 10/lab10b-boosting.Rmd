---
title: "Lab 10b - Boosting"
author: Shih-Ting (Cindy) Huang
date: 03/24/2022
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = T, include  = T, warning=F, message=F)
```

# Learning goals

- Perform gradient boosting and extreme gradient boosting on the ``heart`` data.
- Compare the performance of the two.

# Lab description

For this lab we will be working with the `heart` dataset that you can download from [here](https://github.com/JSC370/jsc370-2022/blob/main/data/heart/heart.csv)


### Setup packages

You should install and load `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r, eval=FALSE, warning=FALSE}
install.packages(c(,"gbm","xgboost","caret"))
```

### Load packages and data
```{r, warning=FALSE, message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(gbm)
library(xgboost)
library(caret)

heart<-read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/heart/heart.csv")
```


---

## Question 1: Gradient Boosting

Evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction).  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

Note, boosting can overfit if the number of trees is too large. The shrinkage parameter controls the rate at which the boosting learns. Very small $\lambda$ can require using a very large number of trees to achieve good performance. Finally, interaction depth controls the interaction order of the boosted model. A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. the default is 1.

i. Split the heart data into training and testing. Also need to make character variables into numeric variables and get rid of missing values.

```{r}
set.seed(301)

# XGBoost isn't NA-friendly, so we omit them from the dataset
# Changing character variables to numeric
heart$AHD_num <- ifelse(heart$AHD=="Yes",1,0)
heart$ChestPain_num <- ifelse(heart$ChestPain=="asymptomatic",1,ifelse(heart$ChestPain=="nonanginal",2,ifelse(heart$ChestPain=="nontypical",3,0)))
heart$Thal_num <- ifelse(heart$Thal=="fixed",1,ifelse(heart$Thal=="normal",2,0))
heart <- heart %>% select(-c(AHD, ChestPain, Thal))
heart <-na.omit(heart)

# Lists of indices
train = sample(1:nrow(heart), floor(nrow(heart) * 0.7))
test = setdiff(1:nrow(heart), train)
```


ii. Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration and calculate the test MSE.

```{r}
set.seed(123)

heart_boost = gbm(AHD_num~., data = heart[train,], distribution="bernoulli", n.trees=5000, shrinkage=0.001, interaction.depth=1, cv.folds=10, class.stratify.cv=T)

summary(heart_boost)

# Train error for every iteration
plot(heart_boost$train.error)
lines(heart_boost$cv.error, col='blue')

yhat_boost <- predict(heart_boost, newdata = heart[test,], n.trees=5000)

mean((yhat_boost-heart[test,"AHD_num"])^2)
```

iii. Repeat ii. using the same seed and ``n.trees=5000`` with the following 3 additional combination of parameters: a) ``shrinkage = 0.001``, ``interaction.depth = 2``; b) ``shrinkage = 0.01``, ``interaction.depth = 1``; c) ``shrinkage = 0.01``, ``interaction.depth = 2``.

(a) yielded an MSE of approximately 3.66, (b)'s is around 11.65, and (c)'s is even higher at 22.26. With a smaller learning rate, the process would be slower, but it seems like it gives us a better result with lower MSE and lowest difference between CV error and training error.

```{r}
set.seed(123)

heart_boost2 = gbm(AHD_num~., data = heart[train,], distribution="bernoulli", n.trees=5000, shrinkage=0.01, interaction.depth=2, cv.folds=10, class.stratify.cv=T)

summary(heart_boost2)

# Train error for every iteration
plot(heart_boost2$train.error)
lines(heart_boost2$cv.error, col='blue')

yhat2_boost <- predict(heart_boost2, newdata = heart[test,], n.trees=5000)

mean((yhat2_boost-heart[test,"AHD_num"])^2)
```


## Question 2: Extreme Gradient Boosting
Training an xgboost model with `xgboost` and perform a grid search for tuning the number of trees and the maxium depth of the tree. Also perform 10-fold cross-validation and determine the variable importance. Finally, compute the test MSE.

```{r, warning=FALSE, message=FALSE, include=FALSE}
# Using caret because it enables us to use grid search on XGBoost
train_control = trainControl(method="cv", number=10, search="grid")

# Create the parameter combinations for Grid Search
tune_grid <- expand.grid(max_depth=c(1,3,5,7),
                         nrounds=(1:10) * 50,
                         eta=c(0.01,0.1,0.3),
                         gamma=0, # minimum amount of reduction
                         subsample=1,
                         min_child_weight=1,
                         colsample_bytree = 0.6 # number of columns that get subsampled
                         )

# Will save the results we've gotten so far even if it didn't finish running
heart_xgb <- caret::train(AHD_num~., data=heart[train,], method="xgbTree",trControl=train_control,tuneGrid=tune_grid)

plot(varImp(heart_xgb,scale=F))

yhat_xgb <- predict(heart_xgb, newdata=heart[test,])

# Calculate MSE
mean((yhat_xgb-heart[test,"AHD_num"])^2)
# Automatically calculates RMSE
caret::RMSE(heart[test,"AHD_num"], yhat_xgb)^2

# Check the final chosen model
heart_xgb$finalModel
```

