---
title: "Lab 08 - Text Mining"
author: "Shih-Ting (Cindy) Huang"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = T, warning=F, echo=F)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text.
- Use dplyr and ggplot2 to analyze text data

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/. And is loaded and "fairly" cleaned at https://github.com/JSC370/jsc370-2022/blob/main/data/medical_transcriptions/mtsamples.csv.

This markdown document should be rendered using `github_document` document.



### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`.
If you don't already have `tidytext` then you can install with

```{r, eval=FALSE}
install.packages("tidytext")
```

### read in Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r, warning=FALSE, message=FALSE}
library(tidytext)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different categories do we have? Are these categories related? overlapping? evenly distributed?

```{r}
mt_samples %>%
  count(medical_specialty, sort = TRUE) %>%
  ggplot(aes(medical_specialty, n)) +
  geom_bar(stat="identity") +
  xlab("Medical Specialty") +
  ylab("Frequency") +
  coord_flip()
```

---

## Question 2

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples %>%
  select(transcription) %>%
  # output col is word, input is transcription
  unnest_tokens(word, transcription) %>%
  group_by(word) %>%
  summarise(word_frequency = n()) %>%
  # arrange by word frequency from most frequent
  arrange(across(word_frequency, desc)) %>%
  head(20)

tokens %>%
  ggplot(aes(x = reorder(word, word_frequency), word_frequency)) +
  # stat means use y column values directly instead of counting
  geom_bar(stat="identity") + coord_flip() +
  ylab("Frequency") +
  xlab("Word") +
  labs(title = "General Word Frequency")

library(wordcloud)
# Creates wordcloud for the most frequent words
wordcloud(tokens$word, tokens$word_frequency)
```

- We don't get a lot of meanignful insights from the results shown above because we have included stopwords (ex. "the", "and", "of", etc.), which appear the most common in the corpus.


---

## Question 3

- Redo visualization but remove stopwords before
- Bonus points if you remove numbers as well

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

```{r}
tokens <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(word, transcription) %>%
  anti_join(stop_words, by="word") %>%
  # match where the whole section is a digit and remove those
  subset(!grepl("^\\d+$", word)) %>%
  group_by(word) %>%
  summarise(word_frequency = n()) %>%
  arrange(across(word_frequency, desc)) %>%
  head(20)
```

```{r}
tokens %>%
  ggplot(aes(x = reorder(word, word_frequency), y = word_frequency)) +
  geom_bar(stat="identity", fill = "#9E788F") +
  ylab("Frequency") +
  xlab("Word") +
  theme_minimal()+
  coord_flip() +
  labs(title = "General Word Frequency (Stopwords Removed)")

wordcloud(tokens$word, tokens$word_frequency)
```


---

# Question 4

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams?
```{r}
tokens_bigram <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(bigram, transcription, token="ngrams", n=2) %>%
  group_by(bigram) %>%
  summarise(bigram_frequency = n()) %>%
  # separate the bigram column into two; if there's more than two words then drop them
  separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep = " ", fill = "right") %>%
  # do on word1 and  word2 separately, "wordx" is the col name in tokens, "word" is the col name in stop_Words
  anti_join(stop_words, by = c("word1"= "word")) %>%
  anti_join(stop_words, by = c("word2"="word")) %>%
  subset(!grepl("\\d+", bigram)) %>%
  arrange(across(bigram_frequency, desc))

```

```{r}
tokens_bigram %>%
  head(20) %>%
  ggplot(aes(x=reorder(bigram, bigram_frequency), y=bigram_frequency)) +
  geom_bar(stat="identity", fill = "#7284A8") + 
  xlab("Frequency") +
  ylab("Bigram") +
  theme_minimal()+
  coord_flip() +
  labs(title = "Bigram Frequency")
```
```{r}
tokens_trigram <- mt_samples %>%
  select(transcription) %>%
  unnest_tokens(trigram, transcription, token="ngrams", n=3) %>%
  group_by(trigram) %>%
  summarise(trigram_frequency = n())

tokens_trigram <- tokens_trigram %>%
  separate(trigram, c("word1", "word2", "word3"), extra="drop", remove=F, sep = " ", fill = "right") %>%
  anti_join(stop_words, by = c("word1"= "word")) %>%
  anti_join(stop_words, by = c("word2"="word")) %>%
  anti_join(stop_words, by = c("word3"="word")) %>%
  # match digit anywhere in the word
  subset(!grepl("\\d+", trigram)) %>%
  arrange(across(trigram_frequency, desc)) 
```

```{r}
tokens_trigram %>%
  head(20) %>%
  ggplot(aes(x=reorder(trigram, trigram_frequency), y=trigram_frequency)) +
  geom_bar(stat="identity", fill = "#474954") + 
  xlab("Frequency") +
  ylab("Trigram") + 
  theme_minimal()+
  coord_flip() +
  labs(title = "Trigram Frequency")
```

---

# Question 5

Using the results you got from question 4. Pick a word and count the words that appears after and before it.
```{r}
tokens_trigram %>% 
  subset(word2 == "blood") %>%
  group_by(word1) %>%
  summarise(word1_freq=n()) %>%
  arrange(across(word1_freq, desc)) %>%
  head(20) %>%
  ggplot(aes(x=reorder(word1, word1_freq), y=word1_freq)) +
  geom_bar(stat="identity", fill = "#252627") +
  ylab("Frequency") +
  xlab("Word 1") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Frequency of Words Preceding 'blood' ")
```

```{r}
tokens_trigram %>% 
  subset(word2 == "blood") %>%
  group_by(word3) %>%
  summarise(word3_freq=n()) %>%
  arrange(across(word3_freq, desc)) %>%
  head(20) %>%
  ggplot(aes(x=reorder(word3, word3_freq), y=word3_freq)) +
  geom_bar(stat="identity", fill = "#A9B3CE") +
  ylab("Frequency") +
  xlab("Word 3") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Frequency of Words After 'blood'")
  
```



---

# Question 6 

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r}
mt_samples %>%
  unnest_tokens(word, transcription) %>%
  anti_join(stop_words, by = "word") %>%
  subset(!grepl("^\\d+$", word)) %>%
  group_by(medical_specialty) %>%
  count(word, sort=T) %>%
  top_n(1,n) %>%
  ggplot(aes(x=medical_specialty, y = n, fill = word)) +
  geom_bar(stat="identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Medical Specialty") +
  theme_minimal()
```


# Question 7 - extra

Find your own insight in the data:

Ideas:

- Interesting ngrams
- See if certain words are used more in some specialties then others

```{r}
word_comp <- mt_samples %>%
  select(transcription, medical_specialty) %>%
  unnest_tokens(word, transcription) %>%
  anti_join(stop_words, by = "word") %>%
  subset(!grepl("^\\d+$", word)) %>%
  subset(word == "patient" | word == "pain" | word =="tumor") %>%
  group_by(word, medical_specialty) %>%
  summarise(word_frequency = n()) %>%
  arrange(across(word_frequency, desc))
```

```{r}
head(word_comp)
```

```{r}
word_comp %>%
  ggplot(aes(medical_specialty, word_frequency)) +
  geom_bar(stat="identity") +
  facet_wrap(~word) +
  xlab("Medical Specialty") +
  ylab("Frequency") +
  coord_flip() +
  labs(title = "Frequency of Specified Words in Medical Specialties")
```
Looking specifically at the words "pain", "patient", and "tumor", we observer that the surgery category has the most mentions of "patient" and "tumor", whereas the consulting category has the most mentions of "pain".

# Deliverables

1. Questions 1-7 answered, pdf or html output uploaded to quercus
